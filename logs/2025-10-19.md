# ğŸ—“ï¸ Daily Log â€“ 2025-10-19

**Focus:** Tokenization in production

## ğŸ§  What I Learned
- Tokenization is usually a preprocessing step in a data pipeline (offline).
- When tokenization becomes a bottleneck:
    - Streaming chat services that tokenize user messages on every turn.
    - Embedding services that chunk and tokenize large documents.
    - LLM gateways that must pre-count tokens for cost and context limits.
- Byte-Pair Encoding (BPE)

## ğŸ’» Snippet / Command
```py
from transformers import AutoTokenizer

# 1. Load the pre-trained tokenizer for a specific model (e.g., "gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "Hello, how are you? Tokenization is cool."

# 2. Tokenize the text. This single call splits the text
#    and converts the tokens into their corresponding IDs.
encoded_output = tokenizer(text)
token_ids = encoded_output['input_ids']

print(f"Original Text:\n{text}\n")
print(f"Token IDs:\n{token_ids}")

print("-" * 20)

# --- Let's look closer at what happened ---

# 3. We can convert the IDs back into tokens to see the subwords
tokens = tokenizer.convert_ids_to_tokens(token_ids)
print(f"Tokens (Subwords):\n{tokens}\n")

# 4. We can also decode the IDs right back into the original string
decoded_text = tokenizer.decode(token_ids)
print(f"Decoded Text:\n{decoded_text}")

outputs:
Original Text:
Hello, how are you? Tokenization is cool.

Token IDs:
[15496, 11, 703, 389, 345, 30, 29130, 1634, 318, 3608, 13]
--------------------
Tokens (Subwords):
['Hello', ',', 'Ä how', 'Ä are', 'Ä you', '?', 'Ä Token', 'ization', 'Ä is', 'Ä cool', '.']

Decoded Text:
Hello, how are you? Tokenization is cool.
```
> What's Happening in This Example?
AutoTokenizer.from_pretrained("gpt2") downloads the specific vocabulary and rules that the GPT-2 model was trained with.tokenizer(text) is the main function. It bundles several steps into one, but the most important part is creating the input_ids.

Notice the tokens output: ['Hello', ',', 'Ä how', 'Ä are', 'Ä you', '?', 'Ä Token', 'ization', 'Ä is', 'Ä cool', '.'].
"Tokenization" was split into Ä Token and ization. This is subword tokenization in action. It allows the model to understand "tokenization" by combining its knowledge of "token" and the suffix "ization".
The Ä  (a special character) represents a space, indicating the start of a new word.


âš ï¸ Speed-Bump / Question
	- How to use caching during tokenization
    - Using Byte-Pair Encoding (BPE) as an example, how long it will take for big dataset.

ğŸ”— Links / References
- https://huggingface.co/learn/llm-course/en/chapter6/5