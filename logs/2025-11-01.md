# üóìÔ∏è Daily Log ‚Äì YYYY-MM-DD

**Focus:** Evaluate the Zero-shot

## üß† What I Learned

- How facebook/bart-large-mnli got trained
  - Stage 1 (Self-Supervised Pre-training): This is how the base bart-large model was created.
  - Stage 2 (Supervised Fine-tuning): This is how it became the mnli model, which specializes in classification.

- Self-Supervised Pre-training (The "bart-large" Base)
  - teaching the model the fundamentals of language: grammar, context, and world knowledge. This is done using a self-supervised method called a **denoising autoencoder**.
  - Here‚Äôs the simple version:
    - Get Data: The model is fed a massive amount of unlabeled text from the internet (like books and Wikipedia).

    - Corrupt Text: It intentionally "breaks" or "corrupts" the text in several ways.

    - Denoise Text: The model's only goal is to reconstruct the original, clean text from the broken version.

- Stage 2: Supervised Fine-tuning (The "mnli" Task)

  - Get Data: The pre-trained bart-large model is given a new, human-labeled dataset called Multi-Genre Natural Language Inference (MNLI).

  - Learn the Task: The MNLI dataset consists of pairs of sentences: a premise and a hypothesis. The model must learn to classify the relationship between them into one of three labels:

  - entailment: The hypothesis is true given the premise.

  - contradiction: The hypothesis is false given the premise.

  - neutral: The hypothesis is neither supported nor contradicted.

Here is a training example:

  - Premise: `A man in a black suit is playing a guitar on a stage.`

  - Hypothesis: `A musician is performing.`

  - Correct Label: `entailment`

- Real-World Applications of Zero-shot:

  - **Content Moderation**: Automatically flagging or filtering content on social media or forums.
  - **Sentiment Analysis**: Quickly gauging the sentiment of customer reviews or social media posts.
  - **Topic Categorization**: Organizing news articles, blog posts, or research papers into predefined topics.
  - **Customer Feedback Analysis**: Sorting feedback into categories like ‚Äúfeature request‚Äù, ‚Äúbug report‚Äù, or ‚Äúpraise.‚Äù

## üíª Snippet / Command

```py
# or C# / SQL / Bash snippet
```

‚ö†Ô∏è Speed-Bump / Question

- what is Denoise Text / denoising autoencoder

üîó Links / References

- https://stackoverflow.com/questions/75866093/how-does-huggingfaces-zero-shot-classification-work-in-production-webapp-do-i

- https://medium.com/@sandyeep70/demystifying-text-summarization-with-deep-learning-ce08d99eda97

- https://www.alphaxiv.org/fr/overview/1910.13461v1

- https://ai.meta.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/#:~:text=Abstract,to%20reconstruct%20the%20original%20text.