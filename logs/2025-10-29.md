# ðŸ—“ï¸ Daily Log â€“ 2025-10-29

**Focus:** Query Understanding Module Design

## ðŸ§  What I Learned

- Normalization & Correction (Preprocessing)
  - Lowercasing/Puncuation Removal/Contraction Expansion/Spelling Correction


## ðŸ’» Snippet / Command

```py
%%writefile text_preprocessor.py
# file: text_preprocessor.py
import re
import html
import unicodedata
import contractions
from spellchecker import SpellChecker
import spacy
from typing import List, Optional


class TextPreprocessor:
    """
    Production-grade NLP preprocessing pipeline.
    Features:
      - HTML/unicode normalization
      - Lowercasing
      - Contraction expansion
      - Punctuation/URL/email removal
      - Tokenization + lemmatization
      - Stopword removal
      - Optional spelling correction
    """

    def __init__(
        self,
        lang_model: str = "en_core_web_sm",
        remove_stopwords: bool = True,
        correct_spelling: bool = False,
        keep_numbers: bool = False,
    ):
        self.remove_stopwords = remove_stopwords
        self.correct_spelling = correct_spelling
        self.keep_numbers = keep_numbers

        # Load spaCy NLP model (disable unused components for speed)
        self.nlp = spacy.load(lang_model, disable=["ner", "parser"])
        self.spell = SpellChecker() if correct_spelling else None

        # Precompile regex patterns
        self.url_pattern = re.compile(r"https?://\S+|www\.\S+")
        self.email_pattern = re.compile(r"\S+@\S+\.\S+")

    # ---------- Main entry point ----------
    def preprocess(self, text: str) -> str:
        text = self._normalize(text)
        text = self._expand_contractions(text)
        tokens = self._tokenize(text)
        tokens = self._lemmatize(tokens)
        tokens = self._filter_tokens(tokens)
        return " ".join(tokens)

    # ---------- Individual steps ----------
    def _normalize(self, text: str) -> str:
      # Expand contractions first (before lowercasing)
      text = contractions.fix(text)
      text = html.unescape(text)
      text = unicodedata.normalize("NFKD", text)
      text = text.lower()
      text = self.url_pattern.sub("", text)
      text = self.email_pattern.sub("", text)
      # keep only letters/numbers
      text = re.sub(r"[^a-z0-9\s]" if self.keep_numbers else r"[^a-z\s]", " ", text)
      text = re.sub(r"\s+", " ", text).strip()
      return text

    def _expand_contractions(self, text: str) -> str:
        return contractions.fix(text)

    def _tokenize(self, text: str):
        return self.nlp(text)

    def _lemmatize(self, tokens):
        return [t.lemma_ for t in tokens]

    def _filter_tokens(self, tokens: List[str]) -> List[str]:
      clean = []
      for token in tokens:
          tok = token.strip()
          if len(tok) <= 1:  # filter out single chars like 't', 's'
              continue
          if self.remove_stopwords and tok in self.nlp.Defaults.stop_words:
              continue
          if not self.keep_numbers and tok.isdigit():
              continue
          if self.correct_spelling:
              tok = self.spell.correction(tok)
          clean.append(tok)
      return clean

outputs:
Original: I can't beleive it's already 2025! Visit https://example.com now.
Processed: I t believe s visit
```

âš ï¸ Speed-Bump / Question

- The Processed result from above output is not ideal. Need debugging

ðŸ”— Links / References

- https://blog.bytebytego.com/p/how-openai-uses-kubernetes-and-apache?utm_campaign=post&utm_medium=web
