# üóìÔ∏è Daily Log ‚Äì YYYY-MM-DD

**Focus:** How to measure embedding models if results are not accurate?

## üß† What I Learned
- How to measure embedding models if results are not accurate?
    - Diagnose Your "Inaccuracy" Type: Type 1: The "Lost" Document (Low Recall)
    - The "Buried" Document (Low MRR/Ranking)
- Recall@K
- ‚Ä¶

## üíª Snippet / Command
```py
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel

# --- 1. Define a Pooling Function ---
# This is the key step that SentenceTransformer automates.
# We will perform mean pooling, which averages the embeddings of all
# tokens in the sentence, using the attention mask to ignore padding.

def mean_pooling(model_output, attention_mask):
    # model_output[0] contains the token embeddings (last_hidden_state)
    # The shape is [batch_size, sequence_length, embedding_dimension]
    token_embeddings = model_output[0] 
    
    # attention_mask is [batch_size, sequence_length]
    # We need to expand it to [batch_size, sequence_length, embedding_dimension]
    # to mask out the padding tokens from the embeddings.
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    
    # Sum the embeddings, but only for non-padding tokens
    # (multiply by 0 for padding tokens, 1 for real tokens)
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    
    # Get the count of non-padding tokens
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    # Divide the sum of embeddings by the count of tokens to get the mean
    return sum_embeddings / sum_mask

# --- 2. Load Model and Tokenizer ---
# We'll use a model that is popular for sentence embeddings.
# 'sentence-transformers/all-MiniLM-L6-v2' is a great, small model.
model_name = 'sentence-transformers/all-MiniLM-L6-v2'
print(f"Loading model: {model_name}")

# AutoTokenizer will load the correct tokenizer for this model.
tokenizer = AutoTokenizer.from_pretrained(model_name)
# AutoModel will load the base transformer model (no specific "head" on top).
model = AutoModel.from_pretrained(model_name)

# Set model to evaluation mode (e.g., disables dropout)
model.eval()

# --- 3. Prepare Input Sentences ---
sentences = [
    "The cat sat on the mat.",
    "A feline was resting on the rug.",
    "Data science is a growing field.",
    "I love to eat fresh pineapple."
]

# --- 4. Tokenize Sentences ---
# padding=True: Pad sentences to the longest length in the batch.
# truncation=True: Truncate sentences that are too long.
# return_tensors='pt': Return PyTorch tensors.
print("Tokenizing sentences...")
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# 'encoded_input' is a dictionary containing:
# - input_ids: The numerical token IDs.
# - attention_mask: A binary mask (1 for real tokens, 0 for [PAD] tokens).
# - token_type_ids: (Not always needed, shows which sentence a token belongs to).

# --- 5. Model Inference (Get Token Embeddings) ---
# We wrap this in torch.no_grad() to disable gradient calculations,
# which saves memory and computation since we're not training.
print("Running model inference...")
with torch.no_grad():
    model_output = model(**encoded_input)

# 'model_output' is an object. 
# model_output[0] or model_output.last_hidden_state contains
# the embeddings for every token in every sentence.
# Shape: [batch_size, sequence_length, hidden_embedding_size]
# For this model: [4, 11, 384] (4 sentences, longest is 11 tokens, 384-dim embedding)

# --- 6. Perform Pooling ---
print("Performing mean pooling...")
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])

# --- 7. Normalize Embeddings ---
# For similarity tasks, embeddings are often normalized to a length of 1 (unit vector).
# This makes cosine similarity calculation equivalent to a simple dot product.
print("Normalizing embeddings...")
normalized_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)

# --- 8. View Results ---
print("\n--- Results ---")
print(f"Batch of sentences shape: {len(sentences)}")
print(f"Token embeddings shape: {model_output.last_hidden_state.shape}")
print(f"Sentence embeddings shape: {normalized_embeddings.shape}")

print("\nFirst sentence embedding (first 5 dimensions):")
print(normalized_embeddings[0][:5])

# --- 9. Use Case: Calculate Cosine Similarity ---
print("\n--- Cosine Similarity Matrix ---")
# Because the embeddings are normalized, we can just use a matrix multiplication
# (dot product) to get the cosine similarity.
similarity_matrix = torch.mm(normalized_embeddings, normalized_embeddings.T)

# Print the matrix
for i in range(len(sentences)):
    for j in range(len(sentences)):
        print(f"{similarity_matrix[i][j]:.4f}", end="\t")
    print(f"<- \"{sentences[i]}\"")

Loading model: sentence-transformers/all-MiniLM-L6-v2
Tokenizing sentences...
Running model inference...
Performing mean pooling...
Normalizing embeddings...

--- Results ---
Batch of sentences shape: 4
Token embeddings shape: torch.Size([4, 11, 384])
Sentence embeddings shape: torch.Size([4, 384])

First sentence embedding (first 5 dimensions):
tensor([ 0.1302, -0.0158, -0.0367,  0.0580, -0.0598])

--- Cosine Similarity Matrix ---
1.0000	0.5631	-0.0092	0.0052	<- "The cat sat on the mat."
0.5631	1.0000	-0.0685	0.0266	<- "A feline was resting on the rug."
-0.0092	-0.0685	1.0000	0.0013	<- "Data science is a growing field."
0.0052	0.0266	0.0013	1.0000	<- "I love to eat fresh pineapple."
```    

‚ö†Ô∏è Speed-Bump / Question
- Something unclear or worth exploring tomorrow

üîó Links / References  
NDCG
- https://www.evidentlyai.com/ranking-metrics/ndcg-metric
- https://nexocode.com/blog/posts/mlops-consulting-services/#:~:text=MLOps%20consulting%20services%20for%20model,not%20detected%20and%20addressed%20quickly.
- https://www.leanware.co/insights/how-much-does-an-ai-consultant-cost

