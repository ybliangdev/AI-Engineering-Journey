# ðŸ—“ï¸ Daily Log â€“ YYYY-MM-DD

**Focus:** Named Entity Recognition

## ðŸ§  What I Learned
- Embedding provides input to NER, An NER model (which is typically a neural network) cannot read the raw text. It needs the text to be converted into numbers first. This is where embeddings come in.
- `spaCy` offers two main types of model architectures for NER:

  CNN Models (e.g., en_core_web_sm, md, lg): Embed (GloVe embedding)-> Encode -> Attend -> Predit



  Transformer Models (e.g., en_core_web_trf): Embed -> Encode -> Predit
- For most production applications where speed is a concern, the lg or md CNN models are a great balance. For tasks where you need the highest possible accuracy, the trf models are the best choice.
- `spacy` provides: tokenization, lemmatization, part-of-speech (POS) tagging, dependency parsing, and named entity recognition (NER) all at once
- `spaCy` vs `HuggingFace`: HuggingFace provides direct access to thousands of massive, pre-trained Transformer models (like BERT, RoBERTa, and GPT-2). These are the absolute state-of-the-art for most NLP tasks but are significantly larger and slower than spaCy's default models.

## ðŸ’» Snippet / Command
Code examples of spaCy:
```py
import spacy

# 1. Load the small English model.
# Note: You may need to run "python -m spacy download en_core_web_sm"
# in your terminal first to download the model.
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("Model 'en_core_web_sm' not found. Please run:")
    print("python -m spacy download en_core_web_sm")
    exit()

# 2. Create a "Doc" object by processing the text.
# This one nlp() call runs the text through the entire pipeline
# (tokenizer, tagger, parser, NER, etc.).
text = "Apple is looking at buying a U.K. startup for $1 billion in 2025."
doc = nlp(text)

print(f"--- Processing Text ---\n{text}\n")

# 3. Tokenization
# The 'doc' object is already a sequence of tokens.
print("--- 1. Tokenization ---")
print("Tokens (and if they are stop words):")
for token in doc:
    # A token object has many attributes
    print(f"  {token.text:<12} (Stop Word: {token.is_stop})")
print("\n")


# 4. Part-of-Speech (POS) Tagging
# Each token has detailed part-of-speech and dependency info.
print("--- 2. Part-of-Speech (POS) Tagging ---")
print("Token | Coarse POS | Fine-grained POS | Syntactic Dependency")
print("-" * 60)
for token in doc:
    # token.pos_ = Simple, universal POS tag
    # token.tag_ = Detailed, language-specific POS tag
    # token.dep_ = The syntactic dependency relation
    print(f"  {token.text:<12} | {token.pos_:<10} | {token.tag_:<18} | {token.dep_}")
print("\n")


# 5. Lemmatization
# The 'lemma' is the base form of the word.
print("--- 3. Lemmatization ---")
print("Word      | Lemma (base form)")
print("-" * 30)
for token in doc:
    # Note: "is" becomes "be", "looking" becomes "look"
    print(f"  {token.text:<10} | {token.lemma_}")
print("\n")


# 6. Named Entity Recognition (NER)
# The 'doc.ents' property holds all entities found by the NER model.
print("--- 4. Named Entity Recognition (NER) ---")
print("Entity Text | Entity Type (Label)")
print("-" * 40)
for ent in doc.ents:
    # ent.text = The text of the entity (e.g., "Apple")
    # ent.label_ = The type of entity (e.g., "ORG" for Organization)
    print(f"  {ent.text:<12} | {ent.label_}")
outputs:
--- Processing Text ---
Apple is looking at buying a U.K. startup for $1 billion in 2025.

--- 1. Tokenization ---
Tokens (and if they are stop words):
  Apple        (Stop Word: False)
  is           (Stop Word: True)
  looking      (Stop Word: False)
  at           (Stop Word: True)
  buying       (Stop Word: False)
  a            (Stop Word: True)
  U.K.         (Stop Word: False)
  startup      (Stop Word: False)
  for          (Stop Word: True)
  $            (Stop Word: False)
  1            (Stop Word: False)
  billion      (Stop Word: False)
  in           (Stop Word: True)
  2025         (Stop Word: False)
  .            (Stop Word: False)


--- 2. Part-of-Speech (POS) Tagging ---
Token | Coarse POS | Fine-grained POS | Syntactic Dependency
------------------------------------------------------------
  Apple        | PROPN      | NNP                | nsubj
  is           | AUX        | VBZ                | aux
  looking      | VERB       | VBG                | ROOT
  at           | ADP        | IN                 | prep
  buying       | VERB       | VBG                | pcomp
  a            | DET        | DT                 | det
  U.K.         | PROPN      | NNP                | dobj
  startup      | NOUN       | NN                 | advcl
  for          | ADP        | IN                 | prep
  $            | SYM        | $                  | quantmod
  1            | NUM        | CD                 | compound
  billion      | NUM        | CD                 | pobj
  in           | ADP        | IN                 | prep
  2025         | NUM        | CD                 | pobj
  .            | PUNCT      | .                  | punct


--- 3. Lemmatization ---
Word      | Lemma (base form)
------------------------------
  Apple      | Apple
  is         | be
  looking    | look
  at         | at
  buying     | buy
  a          | a
  U.K.       | U.K.
  startup    | startup
  for        | for
  $          | $
  1          | 1
  billion    | billion
  in         | in
  2025       | 2025
  .          | .


--- 4. Named Entity Recognition (NER) ---
Entity Text | Entity Type (Label)
----------------------------------------
  Apple        | ORG
  U.K.         | GPE
  $1 billion   | MONEY
  2025         | DATE

```

âš ï¸ Speed-Bump / Question
- What are stop words?


ðŸ”— Links / References
- https://medium.com/@kanerika/named-entity-recognition-a-comprehensive-guide-to-nlps-key-technology-636a124eaa46
- https://medium.com/@bragadeeshs/meet-spacy-the-magical-robot-that-understands-words-efa3f79780bb

