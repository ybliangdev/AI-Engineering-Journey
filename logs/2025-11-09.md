# üóìÔ∏è Daily Log ‚Äì 2025-11-09

**Focus:** Gradient Descent

## üß† What I Learned

- Gradient descent is an optimization algorithm used to find the minimum value of a function. Gradient descent is a training-only algorithm.

- Gradient descent challange: local minimum vs global minium
  - Solution: using STOCHASTIC Gradient Descent (SGD)
  - Random initilization & multiple trail

- In gradient descent, the learning rate is a small number that controls how big of a step you take to get down the hill (the cost function).  
  - Choosing the right learning rate is a trade-off. It directly controls both the speed and the success of your model's training.
  - A good learning rate takes confident steps large enough to make progress quickly, but small enough that it doesn't overshoot the lowest point. The Result: The model finds the minimum error efficiently. This is called convergence.

- Mini-Batch Gradient Descent:
  - Uses a small group (a "mini-batch") of training samples (e.g., 32, 64, or 128 samples) to calculate the gradient for a single step.

## üíª Snippet / Command

```py
# or C# / SQL / Bash snippet
```

‚ö†Ô∏è Speed-Bump / Question


üîó Links / References

- https://medium.com/@shazanansar/escaping-the-trap-of-local-minima-why-optimization-algorithms-get-stuck-and-how-to-break-free-6e3042a6d9c9


MLOps: Continuous delivery and automation pipelines in machine learning
- https://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#:~:text=Model%20evaluation%3A%20The%20model%20is,better%20than%20a%20certain%20baseline.