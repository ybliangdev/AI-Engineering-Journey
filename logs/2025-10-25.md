# üóìÔ∏è Daily Log ‚Äì YYYY-MM-DD

**Focus:** Deploy a simplest embedding model

## üß† What I Learned
- LocalHost/Self-host in cloud/ Open Services
- Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models (quickstart), to calculate similarity scores using Cross-Encoder (a.k.a. reranker) models (quickstart), or to generate sparse embeddings using Sparse Encoder models (quickstart). This unlocks a wide range of applications, including semantic search, semantic textual similarity, and paraphrase mining.
- SentenceTransformer("all-MiniLM-L6-v2") creates an object that loads a specific, machine learning model. Its entire job is to read a piece of text (like a sentence or paragraph) and convert it into a 384-number list (a "vector" or "embedding") that numerically represents the text's semantic meaning.
- How did the model learn to put similar sentences close together? It was fine-tuned using a contrastive learning objective:
1. Get Data: The model was given a massive dataset of sentence pairs. For example:

    - Positive Pair (similar): ("How do I fly a kite?", "A guide to kite flying.")

    - Negative Pair (dissimilar): ("How do I fly a kite?", "What's for dinner?")

2. Process Pairs: It processed both sentences in a pair through the same MiniLM model (this is often called a Siamese Network).

3. Calculate "Closeness": It then calculated the similarity (e.g., cosine similarity) between the two resulting sentence embeddings.

4. Learn: The model's "goal" was to adjust its internal weights to:

    - Maximize the similarity score for positive pairs (pull them together).

    - Minimize the similarity score for negative pairs (push them apart).

- CPU are designed to run multiple different app concurrently.
    - Significant proprotion of energy is used for data movements.i.e. low performance per watt.
    - cache misses also detrimental to performance
- GPU has very efficient performance per watt.Has high bandwidth main memory.

## üíª Snippet / Command
Below code snippets from: https://www.sbert.net
```py
from sentence_transformers import SentenceTransformer

# 1. Load a pretrained Sentence Transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# The sentences to encode
sentences = [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium.",
]

# 2. Calculate embeddings by calling model.encode()
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 384]

# 3. Calculate the embedding similarities
similarities = model.similarity(embeddings, embeddings)
print(similarities)
# tensor([[1.0000, 0.6660, 0.1046],
#         [0.6660, 1.0000, 0.1411],
#         [0.1046, 0.1411, 1.0000]])
```

‚ö†Ô∏è Speed-Bump / Question
- Why do we need a Pooling Layer?

üîó Links / References  
the paper of Sentence-BERT:
- https://arxiv.org/abs/1908.10084  
ONNX:
- https://onnxruntime.ai/
