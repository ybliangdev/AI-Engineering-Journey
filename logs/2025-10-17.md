# ğŸ—“ï¸ Daily Log â€“ 2025-10-17

**Focus:** LLM basic

## ğŸ§  What I Learned
- Tokenization
    - Tokenization means breaking text into smaller pieces called tokens.
    - Limitations of Word Tokenization
      1. Large memory space required as each word has a number
      2. Out-of-Vocabulary (OOV) Words
      3. inefficient becase words have Morphology
        e.g.: "run," "running," "runs," and "ran" are all related, but a simple word tokenizer views them as four separate, unrelated tokens. 

## ğŸ’» Snippet / Command
```py
import tiktoken

enc = tiktoken.get_encoding("cl100k_base")
tokens = enc.encode("ChatGPT is great!")
print(tokens)           # [36781, 318, 374, 0... etc.]
print(len(tokens))
outputs:  
[16047, 38, 2898, 374, 2294, 0]
6
```

âš ï¸ Speed-Bump / Question
- modern NLP models

ğŸ”— Links / References
- https://wandb.ai/mostafaibrahim17/ml-articles/reports/An-introduction-to-tokenization-in-natural-language-processing--Vmlldzo3NTM4MzE5
	
